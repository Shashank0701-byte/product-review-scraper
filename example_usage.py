#!/usr/bin/env python3
"""
Example usage of the Product Review Scraper
This script demonstrates how to use the scraper with different configurations.
"""

import subprocess
import time
import os
from datetime import datetime


def run_example_scraping():
    """Run example scraping sessions with different configurations."""
    
    print("ğŸ” Product Review Scraper - Example Usage")
    print("=" * 60)
    
    # Example URLs for different sites
    examples = [
        {
            "name": "Amazon Product (Small dataset)",
            "url": "https://www.amazon.com/dp/B08N5WRWNW",
            "max_reviews": 50,
            "format": "json"
        },
        {
            "name": "eBay Product (CSV output)",
            "url": "https://www.ebay.com/itm/123456789",  # Replace with real URL
            "max_reviews": 30,
            "format": "csv"
        }
    ]
    
    for i, example in enumerate(examples, 1):
        print(f"\nğŸ“‹ Example {i}: {example['name']}")
        print(f"URL: {example['url']}")
        print(f"Max Reviews: {example['max_reviews']}")
        print(f"Format: {example['format']}")
        
        # Validate URL first
        print("\nğŸ” Validating URL...")
        validate_cmd = [
            "python", "run_scraper.py",
            "--validate-only",
            "--url", example['url']
        ]
        
        try:
            result = subprocess.run(validate_cmd, capture_output=True, text=True, check=True)
            print("âœ… URL validation successful")
        except subprocess.CalledProcessError as e:
            print(f"âŒ URL validation failed: {e.stderr}")
            continue
        
        # Ask user if they want to run this example
        user_input = input(f"\nğŸ¤” Run Example {i}? (y/n/skip all): ").lower().strip()
        
        if user_input == 'skip all':
            break
        elif user_input != 'y':
            print("â­ï¸ Skipping this example")
            continue
        
        # Run the scraper
        print(f"\nğŸš€ Running scraper for Example {i}...")
        scrape_cmd = [
            "python", "run_scraper.py",
            "--url", example['url'],
            "--max-reviews", str(example['max_reviews']),
            "--format", example['format']
        ]
        
        start_time = time.time()
        try:
            result = subprocess.run(scrape_cmd, check=True)
            duration = time.time() - start_time
            print(f"âœ… Example {i} completed in {duration:.1f} seconds")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Example {i} failed: {e}")
        
        print("-" * 60)
    
    print("\nğŸ‰ Example session completed!")
    print("ğŸ“ Check the 'scraped_data' folder for results")


def show_help():
    """Show help information about the scraper."""
    print("\nğŸ“š Product Review Scraper Help")
    print("=" * 60)
    
    print("\nğŸ”§ Basic Usage:")
    print("python run_scraper.py --url 'PRODUCT_URL'")
    
    print("\nğŸ”§ Advanced Usage:")
    print("python run_scraper.py --url 'PRODUCT_URL' --max-reviews 500 --format csv")
    
    print("\nğŸŒ Supported Sites:")
    sites = [
        "Amazon (amazon.com, amazon.co.uk, etc.)",
        "eBay (ebay.com, ebay.co.uk, etc.)",
        "Walmart (walmart.com)",
        "Target (target.com)",
        "Best Buy (bestbuy.com)",
        "Alibaba/AliExpress",
        "Generic e-commerce sites"
    ]
    
    for site in sites:
        print(f"  âœ… {site}")
    
    print("\nğŸ“„ Output Data:")
    fields = [
        "Product name and URL",
        "Review text and title",
        "Rating (numerical)",
        "Review date",
        "Reviewer name",
        "Helpful votes count",
        "Verified purchase status",
        "Scraping metadata"
    ]
    
    for field in fields:
        print(f"  ğŸ“Š {field}")
    
    print("\nâš™ï¸ Configuration Options:")
    options = [
        "--max-reviews: Number of reviews to scrape (default: 200)",
        "--format: Output format - json, csv, or xml (default: json)",
        "--validate-only: Just validate URL without scraping"
    ]
    
    for option in options:
        print(f"  ğŸ”§ {option}")


def main():
    """Main function."""
    print("ğŸ•·ï¸ Product Review Scraper")
    print("A powerful tool for extracting product reviews from e-commerce sites")
    print("=" * 70)
    
    while True:
        print("\nğŸ“‹ Choose an option:")
        print("1. Run example scraping sessions")
        print("2. Show help and usage information")
        print("3. Custom scraping (enter your own URL)")
        print("4. Exit")
        
        choice = input("\nğŸ‘‰ Enter your choice (1-4): ").strip()
        
        if choice == '1':
            run_example_scraping()
        elif choice == '2':
            show_help()
        elif choice == '3':
            url = input("\nğŸ”— Enter product URL: ").strip()
            if url:
                max_reviews = input("ğŸ“„ Max reviews (default 200): ").strip() or "200"
                format_choice = input("ğŸ“‹ Format (json/csv/xml, default json): ").strip() or "json"
                
                cmd = [
                    "python", "run_scraper.py",
                    "--url", url,
                    "--max-reviews", max_reviews,
                    "--format", format_choice
                ]
                
                try:
                    subprocess.run(cmd, check=True)
                    print("âœ… Scraping completed!")
                except subprocess.CalledProcessError as e:
                    print(f"âŒ Scraping failed: {e}")
            else:
                print("âŒ No URL provided")
        elif choice == '4':
            print("\nğŸ‘‹ Thanks for using Product Review Scraper!")
            break
        else:
            print("âŒ Invalid choice. Please enter 1, 2, 3, or 4.")


if __name__ == "__main__":
    main()